{"cells":[{"cell_type":"code","source":["#@title License\n","# Copyright 2022 The Pix2Seq Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# =============================================================================="],"metadata":{"id":"AibS_rGOftPV","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29u50YeGfgLo","executionInfo":{"status":"ok","timestamp":1666598658360,"user_tz":-210,"elapsed":28509,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"3b40a55a-0756-4e85-8c97-9018910da59b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Pix2seq: A Language Modeling Framework for Object Detection\n","<a href=\"https://colab.research.google.com/github/google-research/pix2seq/blob/master/colabs/pix2seq_finetuning_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","\n","\n","This colab presents a demo for object detection fine-tuning with Pix2seq. The table below provides a summary and model location for pretrained models on Objects365 dataset, which can be used as initializations for fine-tuning.\n","\n","Backbone       | Total params (M) | Image size | Google cloud storage location\n","-------------: | ---------------: | ---------: | -----------:\n","ResNet-50      | 36.6             | 640x640    | [gs://pix2seq/obj365_pretrain/resnet_640x640_b256_s400k](https://console.cloud.google.com/storage/browser/pix2seq/obj365_pretrain/resnet_640x640_b256_s400k)\n","ResNet-50 (C4) | 84.7             | 640x640    | [gs://pix2seq/obj365_pretrain/resnetc_640x640_b256_s400k](https://console.cloud.google.com/storage/browser/pix2seq/obj365_pretrain/resnetc_640x640_b256_s400k)\n","ViT-L          | 115.2            | 640x640    | [gs://pix2seq/obj365_pretrain/vit_b_640x640_b256_s400k](https://console.cloud.google.com/storage/browser/pix2seq/obj365_pretrain/vit_b_640x640_b256_s400k)\n","ViT-B          | 341.2            | 640x640    | [gs://pix2seq/obj365_pretrain/vit_l_640x640_b256_s400k](https://console.cloud.google.com/storage/browser/pix2seq/obj365_pretrain/vit_l_640x640_b256_s400k)\n"],"metadata":{"id":"8stlocq-fuYO"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Matority"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3MW_x8-4BJ3j","executionInfo":{"status":"ok","timestamp":1666601193975,"user_tz":-210,"elapsed":11,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"4c5be319-f0b4-451e-a323-113bb8d69bb9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Matority\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5471,"status":"ok","timestamp":1666601199437,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"},"user_tz":-210},"id":"K8sH-7fZpAjW","outputId":"75f77efb-8cc5-4db7-f009-c8d710fde7f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ml_collections in /usr/local/lib/python3.7/dist-packages (0.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ml_collections) (1.15.0)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from ml_collections) (0.5.5)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from ml_collections) (6.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from ml_collections) (1.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.18.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n"]}],"source":["# pip installs.\n","!pip install ml_collections\n","!pip install tensorflow-addons\n","# !git clone https://github.com/google/pix2seq.git\n","\n","import os, sys\n","\n","sys.path.append(os.getcwd())\n","root_dir = os.getcwd()\n","sys.path.insert(1, 'pix2seq')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Matority/pix2seq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDG5BpIpf5Jv","executionInfo":{"status":"ok","timestamp":1666601199438,"user_tz":-210,"elapsed":17,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"cc18e2dc-f693-4715-9935-004267cbcb08"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Matority/pix2seq\n"]}]},{"cell_type":"code","source":["\n","import os, sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from PIL import Image\n","import requests\n","import json\n","\n","import ml_collections\n","import utils\n","from data.dataset import Dataset\n","from models import model as model_lib\n","from models import ar_model\n","from tasks import task as task_lib\n","from tasks import object_detection\n","\n","# Define a Dataset class to use for finetuning.\n","class VocDataset(Dataset):\n","\n","  def extract(self, example, training):\n","    \"\"\"Extracts needed features & annotations into a flat dictionary.\n","\n","    Note: be consisous about 0 in label, which should probably reserved for\n","       special use (such as padding).\n","\n","    Args:\n","      example: `dict` of raw features.\n","      training: `bool` of training vs eval mode.\n","\n","    Returns:\n","      example: `dict` of relevant features and labels\n","    \"\"\"\n","\n","\n","    feature_description = {\n","        'image/encoded': tf.io.VarLenFeature(tf.string),\n","        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n","        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n","        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n","        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n","        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n","      }\n","\n","    def _parse_function(example_proto):\n","      # Parse the input `tf.train.Example` proto using the dictionary above.\n","      return tf.io.parse_single_example(example_proto, feature_description)\n","\n","    parsed = _parse_function(example)\n","    dense_img = tf.sparse.to_dense(parsed['image/encoded'])\n","    # print(tf.image.decode_image(dense_img, dtype=tf.float32))\n","    decoded_img = tf.io.decode_jpeg(dense_img[0], channels = 3)\n","\n","\n","    features = {\n","        'image': tf.image.convert_image_dtype(decoded_img, tf.float32),\n","        'image/id': 0, # dummy int.\n","    }\n","\n","    # The following labels are needed by the object detection task.\n","    label = tf.sparse.to_dense(parsed['image/object/class/label']) + 1  # 0 is reserved for padding.\n","    xmax = tf.sparse.to_dense(parsed['image/object/bbox/xmax'])\n","    xmin = tf.sparse.to_dense(parsed['image/object/bbox/xmin'])\n","    ymax = tf.sparse.to_dense(parsed['image/object/bbox/ymax'])\n","    ymin = tf.sparse.to_dense(parsed['image/object/bbox/ymin'])\n","    bbox = tf.stack([ymin, xmin, ymax, xmax], axis=1)\n","    # print(bbox)\n","\n","    # Use tf.numpy_function to get features not easily computed in tf.\n","    def get_area(bboxes):\n","      return np.asarray([\n","          (b[2] - b[0]) * (b[3] - b[1]) for b in bboxes], dtype=np.float32)\n","\n","    areas = tf.numpy_function(get_area, (bbox,), (tf.float32,))\n","    areas = tf.reshape(areas, [tf.shape(label)[0]])\n","\n","    labels = {\n","        'label': label,\n","        # 'xmax': xmax,\n","        # 'xmin': xmin,\n","        # 'ymax': ymax,\n","        # 'ymin': ymin,\n","        'bbox': bbox,\n","        'area': areas,\n","        'is_crowd': tf.zeros_like(label, tf.bool),\n","    }\n","\n","\n","    # features = {\n","    #     'image': tf.image.convert_image_dtype(example['image'], tf.float32),\n","    #     'image/id': 0, # dummy int.\n","    # }\n","\n","    # # The following labels are needed by the object detection task.\n","    # label = example['objects']['label'] + 1  # 0 is reserved for padding.\n","    # bbox = example['objects']['bbox']\n","\n","    # # Use tf.numpy_function to get features not easily computed in tf.\n","    # def get_area(bboxes):\n","    #   return np.asarray([\n","    #       (b[2] - b[0]) * (b[3] - b[1]) for b in bboxes], dtype=np.int32)\n","\n","    # areas = tf.numpy_function(get_area, (bbox,), (tf.int32,))\n","    # areas = tf.reshape(areas, [tf.shape(label)[0]])\n","\n","    # labels = {\n","    #     'label': label,\n","    #     'bbox': bbox,\n","    #     'area': areas,\n","    #     'is_crowd': tf.zeros_like(label, tf.bool),\n","    # }\n","    \n","    return features, labels\n","\n"],"metadata":{"id":"Fn7sS5YzodyC","executionInfo":{"status":"ok","timestamp":1666601203106,"user_tz":-210,"elapsed":3673,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","# Load config for the pretrained model.\n","pretrained_model_dir = 'gs://pix2seq/obj365_pretrain/resnet_640x640_b256_s400k/'\n","with tf.io.gfile.GFile(os.path.join(pretrained_model_dir, 'config.json'), 'r') as f:\n","  config = ml_collections.ConfigDict(json.loads(f.read()))\n","\n","\n","# loaded_dataset = tf.data.TFRecordDataset(\"/content/drive/MyDrive/Matority/data/color_fashion_tfrec_train\")\n","\n","\n","# Update config for finetuning (some configs were missing at initial pretraining time).\n","config.dataset.tfds_name = 'voc'\n","# config.dataset.data_dir = \"/content/drive/MyDrive/Matority/data/color_fashion_tfrec_train\"\n","config.dataset.batch_duplicates = 1\n","config.dataset.coco_annotations_dir = '/content/drive/MyDrive/Matority/data'\n","config.dataset.train_filename = 'train_anno.json'\n","config.dataset.val_filename = 'test_anno.json'\n","config.training = True\n","config.task.name == 'object_detection'\n","config.task.vocab_id = 10  # object_detection task vocab id.\n","config.task.weight = 1.\n","config.task.max_instances_per_image_test = 10\n","config.tasks = [config.task]\n","config.train.batch_size = 8\n","config.model.name = 'encoder_ar_decoder'  # name of model and trainer in registries.\n","config.model.pretrained_ckpt = pretrained_model_dir\n","config.optimization.learning_rate = 1e-4\n","config.optimization.warmup_steps = 10\n","\n","# Use a smaller image_size to speed up finetuning here.\n","# You can use any image_size of choice.\n","config.model.image_size = 600\n","config.task.image_size = 600\n"],"metadata":{"id":"5f0uNHKUolcV","executionInfo":{"status":"ok","timestamp":1666601203985,"user_tz":-210,"elapsed":889,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Perform training for 1000 steps. This takes about ~20 minutes on a regular Colab GPU.\n","train_steps = 1000\n","use_tpu = False  # Set this accordingly.\n","steps_per_loop = 10\n","tf.config.run_functions_eagerly(False)\n","\n","strategy = utils.build_strategy(use_tpu=use_tpu, master='')\n","\n","# The following snippets are mostly copied and simplified from run.py.\n","with strategy.scope():\n","  # Get dataset.\n","\n","  dataset = VocDataset(config)\n","  \n","  \n","  tmp_dataset = tf.data.TFRecordDataset(\"/content/drive/MyDrive/Matority/data/color_fashion_tfrec_train\")\n","  num_train_examples = 0\n","  for i in tmp_dataset:\n","    num_train_examples += 1\n","  \n","  # Get task.\n","  task = task_lib.TaskRegistry.lookup(config.task.name)(config)\n","  tasks = [task]\n","\n","  # Create tf.data.Dataset.\n","  ds = dataset.pipeline(\n","      process_single_example=task.preprocess_single,\n","      global_batch_size=config.train.batch_size,\n","      training=True)\n","  datasets = [ds]\n","  \n","  print(\"Data Pipeline Created!\")\n","  \n","  # Setup training elements.\n","  trainer = model_lib.TrainerRegistry.lookup(config.model.name)(\n","      config, model_dir='model_dir',\n","      num_train_examples=num_train_examples, train_steps=train_steps)\n","  data_iterators = [iter(dataset) for dataset in datasets]\n","\n","  print(\"Data Iterators Created!\")\n","\n","  @tf.function\n","  def train_multiple_steps(data_iterators, tasks):\n","    train_step = lambda xs, ts=tasks: trainer.train_step(xs, ts, strategy)\n","    for _ in tf.range(steps_per_loop):  # using tf.range prevents unroll.\n","      with tf.name_scope(''):  # prevent `while_` prefix for variable names.\n","        strategy.run(train_step, ([next(it) for it in data_iterators],))\n","\n","  global_step = trainer.optimizer.iterations\n","  cur_step = global_step.numpy()\n","  while cur_step < train_steps:\n","    train_multiple_steps(data_iterators, tasks)\n","    cur_step = global_step.numpy()\n","    print(f\"Done training {cur_step} steps.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rC_5-4DJoo65","outputId":"740e532d-a7f6-430f-ac6f-8dd198ca040c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=0.04s)\n","creating index...\n","index created!\n","Pre-extract\n","Post-extract\n","Data Pipeline Created!\n","Data Iterators Created!\n","Forward pass started\n","[(<tf.Tensor 'IteratorGetNext:0' shape=(8, 600, 600, 3) dtype=float32>, <tf.Tensor 'strided_slice_1:0' shape=(8, 500) dtype=int64>, <tf.Tensor 'strided_slice_2:0' shape=(8, 500) dtype=int64>, <tf.Tensor 'SelectV2_6:0' shape=(8, 500) dtype=float32>)]\n","before logits\n","after logits\n","Forward pass finished\n","Optimiziation started\n","Optimiziation finished\n","Forward pass started\n","[(<tf.Tensor 'IteratorGetNext:0' shape=(8, 600, 600, 3) dtype=float32>, <tf.Tensor 'strided_slice_1:0' shape=(8, 500) dtype=int64>, <tf.Tensor 'strided_slice_2:0' shape=(8, 500) dtype=int64>, <tf.Tensor 'SelectV2_6:0' shape=(8, 500) dtype=float32>)]\n","before logits\n","after logits\n","Forward pass finished\n","Optimiziation started\n","Optimiziation finished\n","Done training 10 steps.\n","Done training 20 steps.\n","Done training 30 steps.\n","Done training 40 steps.\n","Done training 50 steps.\n","Done training 60 steps.\n","Done training 70 steps.\n","Done training 80 steps.\n","Done training 90 steps.\n","Done training 100 steps.\n","Done training 110 steps.\n","Done training 120 steps.\n","Done training 130 steps.\n","Done training 140 steps.\n","Done training 150 steps.\n","Done training 160 steps.\n","Done training 170 steps.\n","Done training 180 steps.\n","Done training 190 steps.\n","Done training 200 steps.\n","Done training 210 steps.\n","Done training 220 steps.\n","Done training 230 steps.\n","Done training 240 steps.\n","Done training 250 steps.\n","Done training 260 steps.\n","Done training 270 steps.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6Sq-xjVvcdj"},"outputs":[],"source":["# Run one step of inference (on the training set).\n","\n","# Set category names in task for visualization.\n","# The category names for COCO are picked up from the coco annotation files. For\n","# other datasets, they can be added manually in the code. If they are missing,\n","# the visualization will not contain category names for predicted boxes, but\n","# no other things will be impacted.\n","# category_names = [\n","#     'Aeroplane', 'Bicycle', 'Bird', 'Boat', 'Bottle', 'Bus', 'Car', 'Cat',\n","#     'Chair', 'Cow', 'Dining table', 'Dog', 'Horse', 'Motorbike', 'People',\n","#     'Potted plant', 'Sheep', 'Sofa', 'Train', 'TV/monitor']\n","category_names = ['none', 'sunglass', 'hat', 'jacket', 'shirt', 'pants', 'shorts',\n","        'skirt', 'dress', 'bag', 'shoe']\n","task._category_names = {\n","    i + 1 : {'name': name} for i, name in enumerate(category_names)}\n","\n","def single_step(examples):\n","  preprocessed_outputs = task.preprocess_batched(examples, training=False)\n","  infer_outputs = task.infer(trainer.model, preprocessed_outputs)\n","  return task.postprocess_tpu(*infer_outputs)\n","\n","records = []\n","eval_steps = 10\n","\n","with strategy.scope():\n","  @tf.function\n","  def run_single_step(iterator):\n","    examples = next(iterator)\n","    outputs = strategy.run(single_step, (examples,))\n","    if outputs is not None:\n","      outputs = [strategy.gather(t, axis=0) for t in outputs]\n","    return outputs\n","\n","  iterator = iter(ds)\n","  while True:\n","      if eval_steps and cur_step >= eval_steps:\n","        break\n","      try:\n","        per_step_outputs = run_single_step(iterator)\n","        vis = task.postprocess_cpu(\n","            per_step_outputs,\n","            train_step=100, # global_step.numpy(),\n","            eval_step=cur_step,\n","            ret_results=True)\n","        \n","        records.append(vis['records'])\n","        cur_step += 1\n","      except tf.errors.OutOfRangeError:\n","        print('Break due to OutOfRangeError exception')\n","        break\n","\n","\n","  # per_step_outputs = run_single_step(iterator)\n","  # vis = task.postprocess_cpu(\n","  #     per_step_outputs,\n","  #     train_step=50,\n","  #     eval_step=0,\n","  #     ret_results=True)\n","  # records.append(vis['records'])\n","\n","\n","for record in records:\n","  for key in record.keys():\n","    record[key] = record[key].numpy()\n","\n","\n","import pickle\n","with open('outputs/out.pickle', 'wb') as f:\n","    pickle.dump(records, f)\n","  "]},{"cell_type":"code","source":["with open('outputs/out.pickle', 'rb') as f:\n","    loaded_obj = pickle.load(f)[0]\n"],"metadata":{"id":"qmBGYJ2JJ7_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(records[0]['prec_classes'])"],"metadata":{"id":"pP29nRmeK2e8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(loaded_obj['prec_classes'])"],"metadata":{"id":"0-QukDGP9FLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# summary_writer = tf.summary.create_file_writer('model_dir/')\n","# eval_tag = config.eval.tag\n","\n","# cur_step = global_step.numpy()\n","# result = task.evaluate(summary_writer, cur_step, eval_tag)\n","# result.update({'global_step': cur_step})\n","# print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1spUWcjPuZ_Y","executionInfo":{"status":"ok","timestamp":1666596408925,"user_tz":-210,"elapsed":352,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"829dda3d-b249-47c9-8351-20e5a88ca3ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'global_step': 0}\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wARhnddcmfKHEqfDtGY5l2hrl1Am94Ol"},"executionInfo":{"elapsed":4678,"status":"ok","timestamp":1666535324961,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"},"user_tz":-210},"id":"QD_rgNA_XQMN","outputId":"aff9256c-537c-46c6-92fc-4dfc4f3a86fb"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Visualization.\n","im = tf.concat([vis['pred'][i] for i in range(config.train.batch_size)], 0)\n","Image.fromarray(np.uint8(im.numpy() * 255))"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Matority/pix2seq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QYP1Fmx357km","executionInfo":{"status":"ok","timestamp":1666593990339,"user_tz":-210,"elapsed":343,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"5c2cbb7d-49f3-490c-a762-5c4dfef5f07b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Matority/pix2seq\n"]}]},{"cell_type":"code","source":["!git checkout master   \n","!git branch main master -f    \n","!git checkout main  \n","!git push origin main -f "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbgQTYdGAYaf","executionInfo":{"status":"ok","timestamp":1666531300443,"user_tz":-210,"elapsed":1548,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"e3b8f36d-743f-43e1-cfdc-e0a6279d14d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["M\tcolabs/Copy of Pix2Seq Finetuning Object Detection.ipynb\n","M\tdata/__pycache__/dataset.cpython-37.pyc\n","M\tmodels/__pycache__/ar_model.cpython-37.pyc\n","M\ttasks/__pycache__/object_detection.cpython-37.pyc\n","Already on 'master'\n","M\tcolabs/Copy of Pix2Seq Finetuning Object Detection.ipynb\n","M\tdata/__pycache__/dataset.cpython-37.pyc\n","M\tmodels/__pycache__/ar_model.cpython-37.pyc\n","M\ttasks/__pycache__/object_detection.cpython-37.pyc\n","Switched to branch 'main'\n","Total 0 (delta 0), reused 0 (delta 0)\n","To https://github.com/mehrdadsaberi/pix2seq-M.git\n"," + 6d45f77...0db54da main -> main (forced update)\n"]}]},{"cell_type":"code","source":["!git init"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJycZ90q2Qxj","executionInfo":{"status":"ok","timestamp":1666529072449,"user_tz":-210,"elapsed":700,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"aa602556-a864-4d6d-9a83-4b3a2854d462"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized empty Git repository in /content/drive/MyDrive/Matority/pix2seq/.git/\n"]}]},{"cell_type":"code","source":["!git remote set-url origin https://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@github.com/mehrdadsaberi/pix2seq-M.git"],"metadata":{"id":"BsR44OkY1i9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"outputs fixed\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aia8lxHz6CoI","executionInfo":{"status":"ok","timestamp":1666593985230,"user_tz":-210,"elapsed":2979,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"5d40851b-1494-48a8-ad4e-a5b84861f2e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: Unable to create '/content/drive/MyDrive/Matority/pix2seq/.git/index.lock': File exists.\n","\n","Another git process seems to be running in this repository, e.g.\n","an editor opened by 'git commit'. Please make sure all processes\n","are terminated then try again. If it still fails, a git process\n","may have crashed in this repository earlier:\n","remove the file manually to continue.\n"]}]},{"cell_type":"code","source":["!git push origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9AsMOjzP6YAg","executionInfo":{"status":"ok","timestamp":1666531357806,"user_tz":-210,"elapsed":1100,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"22f9f04e-bd74-4524-bcf7-3d58f814dc5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Counting objects: 13, done.\n","Delta compression using up to 2 threads.\n","Compressing objects:   7% (1/13)   \rCompressing objects:  15% (2/13)   \rCompressing objects:  23% (3/13)   \rCompressing objects:  30% (4/13)   \rCompressing objects:  38% (5/13)   \rCompressing objects:  46% (6/13)   \rCompressing objects:  53% (7/13)   \rCompressing objects:  61% (8/13)   \rCompressing objects:  69% (9/13)   \rCompressing objects:  76% (10/13)   \rCompressing objects:  84% (11/13)   \rCompressing objects:  92% (12/13)   \rCompressing objects: 100% (13/13)   \rCompressing objects: 100% (13/13), done.\n","Writing objects:   7% (1/13)   \rWriting objects:  15% (2/13)   \rWriting objects:  23% (3/13)   \rWriting objects:  30% (4/13)   \rWriting objects:  38% (5/13)   \rWriting objects:  46% (6/13)   \rWriting objects:  53% (7/13)   \rWriting objects:  61% (8/13)   \rWriting objects:  69% (9/13)   \rWriting objects:  76% (10/13)   \rWriting objects:  84% (11/13)   \rWriting objects:  92% (12/13)   \rWriting objects: 100% (13/13)   \rWriting objects: 100% (13/13), 13.25 KiB | 1.66 MiB/s, done.\n","Total 13 (delta 7), reused 0 (delta 0)\n","remote: Resolving deltas:   0% (0/7)\u001b[K\rremote: Resolving deltas:  14% (1/7)\u001b[K\rremote: Resolving deltas:  28% (2/7)\u001b[K\rremote: Resolving deltas:  42% (3/7)\u001b[K\rremote: Resolving deltas:  57% (4/7)\u001b[K\rremote: Resolving deltas:  71% (5/7)\u001b[K\rremote: Resolving deltas:  85% (6/7)\u001b[K\rremote: Resolving deltas: 100% (7/7)\u001b[K\rremote: Resolving deltas: 100% (7/7), completed with 7 local objects.\u001b[K\n","To https://github.com/mehrdadsaberi/pix2seq-M.git\n","   0db54da..b3b0e9c  main -> main\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"merhdads@gmail.com\"\n","!git config --global user.name \"mehrdadsaberi\""],"metadata":{"id":"A9jJpDRE6N_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -rf ~"],"metadata":{"id":"ytC94MHDvxXf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add . -v"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kvOXx4P_4nWS","executionInfo":{"status":"ok","timestamp":1666594137892,"user_tz":-210,"elapsed":344,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"a6872d97-0294-499c-b37b-4c71122c9fad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: not a git repository (or any parent up to mount point /content)\n","Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rb5crypyKn3v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666529641783,"user_tz":-210,"elapsed":348,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"f502fea7-3890-4ede-dce1-77af9e544dd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["105K\tarchitectures\n","3.3M\tcolabs\n","22K\tconfigs\n","1.5K\tCONTRIBUTING.md\n","81K\tdata\n","12K\tLICENSE\n","22K\tmetrics\n","55K\tmodels\n","12M\tpix2seq.gif\n","237K\tpix2seq.png\n","16K\t__pycache__\n","8.0K\tREADME.md\n","1.5K\tregistry.py\n","512\trequirements.txt\n","9.5K\trun.py\n","7.0K\tsample_run.py\n","215K\ttasks\n","12K\tutils.py\n","2.0K\tvocab.py\n","16M\ttotal\n"]}],"source":["!du -shc *"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"last_runtime":{},"provenance":[{"file_id":"https://github.com/google-research/pix2seq/blob/master/colabs/pix2seq_finetuning_object_detection.ipynb","timestamp":1666522575061}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}