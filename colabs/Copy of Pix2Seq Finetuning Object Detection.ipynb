{"cells":[{"cell_type":"code","source":["#@title License\n","# Copyright 2022 The Pix2Seq Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# =============================================================================="],"metadata":{"id":"AibS_rGOftPV","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29u50YeGfgLo","executionInfo":{"status":"ok","timestamp":1666609179364,"user_tz":-210,"elapsed":23271,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"7d1bee61-a0a4-472e-99f5-e326fbbe279b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Pix2seq: A Language Modeling Framework for Object Detection\n","<a href=\"https://colab.research.google.com/github/google-research/pix2seq/blob/master/colabs/pix2seq_finetuning_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","\n","\n","This colab presents a demo for object detection fine-tuning with Pix2seq. The table below provides a summary and model location for pretrained models on Objects365 dataset, which can be used as initializations for fine-tuning.\n","\n","Backbone       | Total params (M) | Image size | Google cloud storage location\n","-------------: | ---------------: | ---------: | -----------:\n","ResNet-50      | 36.6             | 640x640    | [gs://pix2seq/obj365_pretrain/resnet_640x640_b256_s400k](https://console.cloud.google.com/storage/browser/pix2seq/obj365_pretrain/resnet_640x640_b256_s400k)\n","ResNet-50 (C4) | 84.7             | 640x640    | [gs://pix2seq/obj365_pretrain/resnetc_640x640_b256_s400k](https://console.cloud.google.com/storage/browser/pix2seq/obj365_pretrain/resnetc_640x640_b256_s400k)\n","ViT-L          | 115.2            | 640x640    | [gs://pix2seq/obj365_pretrain/vit_b_640x640_b256_s400k](https://console.cloud.google.com/storage/browser/pix2seq/obj365_pretrain/vit_b_640x640_b256_s400k)\n","ViT-B          | 341.2            | 640x640    | [gs://pix2seq/obj365_pretrain/vit_l_640x640_b256_s400k](https://console.cloud.google.com/storage/browser/pix2seq/obj365_pretrain/vit_l_640x640_b256_s400k)\n"],"metadata":{"id":"8stlocq-fuYO"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Matority"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3MW_x8-4BJ3j","executionInfo":{"status":"ok","timestamp":1666609212377,"user_tz":-210,"elapsed":543,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"67fe5a8d-c29f-496f-ef6d-f33cc5bc6988"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Matority\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9559,"status":"ok","timestamp":1666609222604,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"},"user_tz":-210},"id":"K8sH-7fZpAjW","outputId":"f335c821-b567-412c-fb32-80d0de5d1449"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ml_collections\n","  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from ml_collections) (1.3.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from ml_collections) (6.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ml_collections) (1.15.0)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from ml_collections) (0.5.5)\n","Building wheels for collected packages: ml-collections\n","  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94524 sha256=e977e3928173b9cf06f46e3f22fad6938e575415a69d9a3648c9e39704d5b96c\n","  Stored in directory: /root/.cache/pip/wheels/b7/da/64/33c926a1b10ff19791081b705879561b715a8341a856a3bbd2\n","Successfully built ml-collections\n","Installing collected packages: ml-collections\n","Successfully installed ml-collections-0.1.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 12.6 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.18.0\n"]}],"source":["# pip installs.\n","!pip install ml_collections\n","!pip install tensorflow-addons\n","# !git clone https://github.com/google/pix2seq.git\n","\n","import os, sys\n","\n","sys.path.append(os.getcwd())\n","root_dir = os.getcwd()\n","sys.path.insert(1, 'pix2seq')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Matority/pix2seq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDG5BpIpf5Jv","executionInfo":{"status":"ok","timestamp":1666609222604,"user_tz":-210,"elapsed":15,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"b46e5cf6-dea3-4ae8-8030-52565ec71d0c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Matority/pix2seq\n"]}]},{"cell_type":"code","source":["\n","import os, sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from PIL import Image\n","import requests\n","import json\n","\n","import ml_collections\n","import utils\n","from data.dataset import Dataset\n","from models import model as model_lib\n","from models import ar_model\n","from tasks import task as task_lib\n","from tasks import object_detection\n","\n","# Define a Dataset class to use for finetuning.\n","class VocDataset(Dataset):\n","\n","  def extract(self, example, training):\n","    \"\"\"Extracts needed features & annotations into a flat dictionary.\n","\n","    Note: be consisous about 0 in label, which should probably reserved for\n","       special use (such as padding).\n","\n","    Args:\n","      example: `dict` of raw features.\n","      training: `bool` of training vs eval mode.\n","\n","    Returns:\n","      example: `dict` of relevant features and labels\n","    \"\"\"\n","\n","\n","    feature_description = {\n","        'image/encoded': tf.io.VarLenFeature(tf.string),\n","        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n","        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n","        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n","        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n","        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n","      }\n","\n","    def _parse_function(example_proto):\n","      # Parse the input `tf.train.Example` proto using the dictionary above.\n","      return tf.io.parse_single_example(example_proto, feature_description)\n","\n","    parsed = _parse_function(example)\n","    dense_img = tf.sparse.to_dense(parsed['image/encoded'])\n","    # print(tf.image.decode_image(dense_img, dtype=tf.float32))\n","    decoded_img = tf.io.decode_jpeg(dense_img[0], channels = 3)\n","\n","\n","    features = {\n","        'image': tf.image.convert_image_dtype(decoded_img, tf.float32),\n","        'image/id': 0, # dummy int.\n","    }\n","\n","    # The following labels are needed by the object detection task.\n","    label = tf.sparse.to_dense(parsed['image/object/class/label']) + 1  # 0 is reserved for padding.\n","    xmax = tf.sparse.to_dense(parsed['image/object/bbox/xmax'])\n","    xmin = tf.sparse.to_dense(parsed['image/object/bbox/xmin'])\n","    ymax = tf.sparse.to_dense(parsed['image/object/bbox/ymax'])\n","    ymin = tf.sparse.to_dense(parsed['image/object/bbox/ymin'])\n","    bbox = tf.stack([ymin, xmin, ymax, xmax], axis=1)\n","    # print(bbox)\n","\n","    # Use tf.numpy_function to get features not easily computed in tf.\n","    def get_area(bboxes):\n","      return np.asarray([\n","          (b[2] - b[0]) * (b[3] - b[1]) for b in bboxes], dtype=np.float32)\n","\n","    areas = tf.numpy_function(get_area, (bbox,), (tf.float32,))\n","    areas = tf.reshape(areas, [tf.shape(label)[0]])\n","\n","    labels = {\n","        'label': label,\n","        # 'xmax': xmax,\n","        # 'xmin': xmin,\n","        # 'ymax': ymax,\n","        # 'ymin': ymin,\n","        'bbox': bbox,\n","        'area': areas,\n","        'is_crowd': tf.zeros_like(label, tf.bool),\n","    }\n","\n","\n","    # features = {\n","    #     'image': tf.image.convert_image_dtype(example['image'], tf.float32),\n","    #     'image/id': 0, # dummy int.\n","    # }\n","\n","    # # The following labels are needed by the object detection task.\n","    # label = example['objects']['label'] + 1  # 0 is reserved for padding.\n","    # bbox = example['objects']['bbox']\n","\n","    # # Use tf.numpy_function to get features not easily computed in tf.\n","    # def get_area(bboxes):\n","    #   return np.asarray([\n","    #       (b[2] - b[0]) * (b[3] - b[1]) for b in bboxes], dtype=np.int32)\n","\n","    # areas = tf.numpy_function(get_area, (bbox,), (tf.int32,))\n","    # areas = tf.reshape(areas, [tf.shape(label)[0]])\n","\n","    # labels = {\n","    #     'label': label,\n","    #     'bbox': bbox,\n","    #     'area': areas,\n","    #     'is_crowd': tf.zeros_like(label, tf.bool),\n","    # }\n","    \n","    return features, labels\n","\n"],"metadata":{"id":"Fn7sS5YzodyC","executionInfo":{"status":"ok","timestamp":1666609235196,"user_tz":-210,"elapsed":12601,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","# Load config for the pretrained model.\n","pretrained_model_dir = 'gs://pix2seq/obj365_pretrain/resnetc_640x640_b256_s400k'\n","with tf.io.gfile.GFile(os.path.join(pretrained_model_dir, 'config.json'), 'r') as f:\n","  config = ml_collections.ConfigDict(json.loads(f.read()))\n","\n","\n","# loaded_dataset = tf.data.TFRecordDataset(\"/content/drive/MyDrive/Matority/data/color_fashion_tfrec_train\")\n","\n","\n","# Update config for finetuning (some configs were missing at initial pretraining time).\n","config.dataset.tfds_name = 'voc'\n","# config.dataset.data_dir = \"/content/drive/MyDrive/Matority/data/color_fashion_tfrec_train\"\n","config.dataset.batch_duplicates = 1\n","config.dataset.coco_annotations_dir = '/content/drive/MyDrive/Matority/data'\n","config.dataset.train_filename = 'train_anno.json'\n","config.dataset.val_filename = 'test_anno.json'\n","config.training = True\n","config.task.name == 'object_detection'\n","config.task.vocab_id = 10  # object_detection task vocab id.\n","config.task.weight = 1.\n","config.task.max_instances_per_image_test = 10\n","config.tasks = [config.task]\n","config.train.batch_size = 2\n","config.model.name = 'encoder_ar_decoder'  # name of model and trainer in registries.\n","config.model.pretrained_ckpt = pretrained_model_dir\n","config.optimization.learning_rate = 1e-4\n","config.optimization.warmup_steps = 10\n","\n","# Use a smaller image_size to speed up finetuning here.\n","# You can use any image_size of choice.\n","config.model.image_size = 600\n","config.task.image_size = 600\n"],"metadata":{"id":"5f0uNHKUolcV","executionInfo":{"status":"ok","timestamp":1666610794815,"user_tz":-210,"elapsed":7,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Perform training for 1000 steps. This takes about ~20 minutes on a regular Colab GPU.\n","train_steps = 1000\n","use_tpu = False  # Set this accordingly.\n","steps_per_loop = 10\n","tf.config.run_functions_eagerly(False)\n","\n","strategy = utils.build_strategy(use_tpu=use_tpu, master='')\n","\n","# The following snippets are mostly copied and simplified from run.py.\n","with strategy.scope():\n","  # Get dataset.\n","\n","  dataset = VocDataset(config)\n","  \n","  \n","  tmp_dataset = tf.data.TFRecordDataset(\"/content/drive/MyDrive/Matority/data/color_fashion_tfrec_train\")\n","  num_train_examples = 0\n","  for i in tmp_dataset:\n","    num_train_examples += 1\n","  \n","  # Get task.\n","  task = task_lib.TaskRegistry.lookup(config.task.name)(config)\n","  tasks = [task]\n","\n","  # Create tf.data.Dataset.\n","  ds = dataset.pipeline(\n","      process_single_example=task.preprocess_single,\n","      global_batch_size=config.train.batch_size,\n","      training=True)\n","  datasets = [ds]\n","  \n","  print(\"Data Pipeline Created!\")\n","  \n","  # Setup training elements.\n","  trainer = model_lib.TrainerRegistry.lookup(config.model.name)(\n","      config, model_dir='model_dir',\n","      num_train_examples=num_train_examples, train_steps=train_steps)\n","  data_iterators = [iter(dataset) for dataset in datasets]\n","\n","  print(\"Data Iterators Created!\")\n","\n","  @tf.function\n","  def train_multiple_steps(data_iterators, tasks):\n","    train_step = lambda xs, ts=tasks: trainer.train_step(xs, ts, strategy)\n","    for _ in tf.range(steps_per_loop):  # using tf.range prevents unroll.\n","      with tf.name_scope(''):  # prevent `while_` prefix for variable names.\n","        strategy.run(train_step, ([next(it) for it in data_iterators],))\n","\n","  global_step = trainer.optimizer.iterations\n","  cur_step = global_step.numpy()\n","  while cur_step < train_steps:\n","    train_multiple_steps(data_iterators, tasks)\n","    cur_step = global_step.numpy()\n","    print(f\"Done training {cur_step} steps.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rC_5-4DJoo65","executionInfo":{"status":"ok","timestamp":1666612523250,"user_tz":-210,"elapsed":1722084,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"778da72f-dfd4-4595-8c07-3677600e187c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=0.02s)\n","creating index...\n","index created!\n","Pre-extract\n","Post-extract\n","Data Pipeline Created!\n","Data Iterators Created!\n","Forward pass started\n","[(<tf.Tensor 'IteratorGetNext:0' shape=(2, 600, 600, 3) dtype=float32>, <tf.Tensor 'strided_slice_1:0' shape=(2, 500) dtype=int64>, <tf.Tensor 'strided_slice_2:0' shape=(2, 500) dtype=int64>, <tf.Tensor 'SelectV2_6:0' shape=(2, 500) dtype=float32>)]\n","before logits\n","after logits\n","Forward pass finished\n","Optimiziation started\n","Optimiziation finished\n","Forward pass started\n","[(<tf.Tensor 'IteratorGetNext:0' shape=(2, 600, 600, 3) dtype=float32>, <tf.Tensor 'strided_slice_1:0' shape=(2, 500) dtype=int64>, <tf.Tensor 'strided_slice_2:0' shape=(2, 500) dtype=int64>, <tf.Tensor 'SelectV2_6:0' shape=(2, 500) dtype=float32>)]\n","before logits\n","after logits\n","Forward pass finished\n","Optimiziation started\n","Optimiziation finished\n","Done training 10 steps.\n","Done training 20 steps.\n","Done training 30 steps.\n","Done training 40 steps.\n","Done training 50 steps.\n","Done training 60 steps.\n","Done training 70 steps.\n","Done training 80 steps.\n","Done training 90 steps.\n","Done training 100 steps.\n","Done training 110 steps.\n","Done training 120 steps.\n","Done training 130 steps.\n","Done training 140 steps.\n","Done training 150 steps.\n","Done training 160 steps.\n","Done training 170 steps.\n","Done training 180 steps.\n","Done training 190 steps.\n","Done training 200 steps.\n","Done training 210 steps.\n","Done training 220 steps.\n","Done training 230 steps.\n","Done training 240 steps.\n","Done training 250 steps.\n","Done training 260 steps.\n","Done training 270 steps.\n","Done training 280 steps.\n","Done training 290 steps.\n","Done training 300 steps.\n","Done training 310 steps.\n","Done training 320 steps.\n","Done training 330 steps.\n","Done training 340 steps.\n","Done training 350 steps.\n","Done training 360 steps.\n","Done training 370 steps.\n","Done training 380 steps.\n","Done training 390 steps.\n","Done training 400 steps.\n","Done training 410 steps.\n","Done training 420 steps.\n","Done training 430 steps.\n","Done training 440 steps.\n","Done training 450 steps.\n","Done training 460 steps.\n","Done training 470 steps.\n","Done training 480 steps.\n","Done training 490 steps.\n","Done training 500 steps.\n","Done training 510 steps.\n","Done training 520 steps.\n","Done training 530 steps.\n","Done training 540 steps.\n","Done training 550 steps.\n","Done training 560 steps.\n","Done training 570 steps.\n","Done training 580 steps.\n","Done training 590 steps.\n","Done training 600 steps.\n","Done training 610 steps.\n","Done training 620 steps.\n","Done training 630 steps.\n","Done training 640 steps.\n","Done training 650 steps.\n","Done training 660 steps.\n","Done training 670 steps.\n","Done training 680 steps.\n","Done training 690 steps.\n","Done training 700 steps.\n","Done training 710 steps.\n","Done training 720 steps.\n","Done training 730 steps.\n","Done training 740 steps.\n","Done training 750 steps.\n","Done training 760 steps.\n","Done training 770 steps.\n","Done training 780 steps.\n","Done training 790 steps.\n","Done training 800 steps.\n","Done training 810 steps.\n","Done training 820 steps.\n","Done training 830 steps.\n","Done training 840 steps.\n","Done training 850 steps.\n","Done training 860 steps.\n","Done training 870 steps.\n","Done training 880 steps.\n","Done training 890 steps.\n","Done training 900 steps.\n","Done training 910 steps.\n","Done training 920 steps.\n","Done training 930 steps.\n","Done training 940 steps.\n","Done training 950 steps.\n","Done training 960 steps.\n","Done training 970 steps.\n","Done training 980 steps.\n","Done training 990 steps.\n","Done training 1000 steps.\n"]}]},{"cell_type":"code","source":["eval_dataset = VocDataset(config)\n","\n","# Create tf.data.Dataset.\n","eval_ds = eval_dataset.pipeline(\n","    process_single_example=task.preprocess_single,\n","    global_batch_size=config.train.batch_size,\n","    training=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9kfE6moTLfN","executionInfo":{"status":"ok","timestamp":1666612523923,"user_tz":-210,"elapsed":699,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"f1cd0aeb-da17-4bed-ebf0-d84fdf0fb532"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Pre-extract\n","Post-extract\n"]}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"y6Sq-xjVvcdj","executionInfo":{"status":"ok","timestamp":1666612792260,"user_tz":-210,"elapsed":268342,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7dbabbb3-952c-498d-fa2d-ce8dd8d57ffb"},"outputs":[{"output_type":"stream","name":"stdout","text":["eval step: 0\n","eval step: 1\n","eval step: 2\n","eval step: 3\n","eval step: 4\n","eval step: 5\n","eval step: 6\n","eval step: 7\n","eval step: 8\n","eval step: 9\n","eval step: 10\n","eval step: 11\n","eval step: 12\n","eval step: 13\n","eval step: 14\n","eval step: 15\n","eval step: 16\n","eval step: 17\n","eval step: 18\n","eval step: 19\n","eval step: 20\n","eval step: 21\n","eval step: 22\n","eval step: 23\n","eval step: 24\n","eval step: 25\n","eval step: 26\n","eval step: 27\n","eval step: 28\n","eval step: 29\n","eval step: 30\n","eval step: 31\n","eval step: 32\n","eval step: 33\n","eval step: 34\n","eval step: 35\n","eval step: 36\n","eval step: 37\n","eval step: 38\n","eval step: 39\n","eval step: 40\n","eval step: 41\n","eval step: 42\n","eval step: 43\n","eval step: 44\n","eval step: 45\n","eval step: 46\n","eval step: 47\n","eval step: 48\n","eval step: 49\n","eval step: 50\n","eval step: 51\n","eval step: 52\n","eval step: 53\n","eval step: 54\n","eval step: 55\n","eval step: 56\n","eval step: 57\n","eval step: 58\n","eval step: 59\n","eval step: 60\n","eval step: 61\n","eval step: 62\n","eval step: 63\n","eval step: 64\n","eval step: 65\n","eval step: 66\n","eval step: 67\n","eval step: 68\n","eval step: 69\n","eval step: 70\n","eval step: 71\n","eval step: 72\n","eval step: 73\n","eval step: 74\n","eval step: 75\n","eval step: 76\n","eval step: 77\n","eval step: 78\n","eval step: 79\n","eval step: 80\n","eval step: 81\n","eval step: 82\n","eval step: 83\n","eval step: 84\n","eval step: 85\n","eval step: 86\n","eval step: 87\n","eval step: 88\n","eval step: 89\n","eval step: 90\n","eval step: 91\n","eval step: 92\n","eval step: 93\n","eval step: 94\n","eval step: 95\n","eval step: 96\n","eval step: 97\n","eval step: 98\n","eval step: 99\n","eval step: 100\n","eval step: 101\n","eval step: 102\n","eval step: 103\n","eval step: 104\n","eval step: 105\n","eval step: 106\n","eval step: 107\n","eval step: 108\n","eval step: 109\n","eval step: 110\n","eval step: 111\n","eval step: 112\n","eval step: 113\n","eval step: 114\n","eval step: 115\n","eval step: 116\n","eval step: 117\n","eval step: 118\n","eval step: 119\n","eval step: 120\n","eval step: 121\n","eval step: 122\n","eval step: 123\n","eval step: 124\n","eval step: 125\n","eval step: 126\n","eval step: 127\n","eval step: 128\n","eval step: 129\n","eval step: 130\n","eval step: 131\n","eval step: 132\n","eval step: 133\n","eval step: 134\n","eval step: 135\n","eval step: 136\n","eval step: 137\n","eval step: 138\n","eval step: 139\n","eval step: 140\n","eval step: 141\n","eval step: 142\n","eval step: 143\n","eval step: 144\n","eval step: 145\n","eval step: 146\n","eval step: 147\n","eval step: 148\n","eval step: 149\n","eval step: 150\n","eval step: 151\n","eval step: 152\n","eval step: 153\n","eval step: 154\n","eval step: 155\n","eval step: 156\n","eval step: 157\n","eval step: 158\n","eval step: 159\n","eval step: 160\n","eval step: 161\n","eval step: 162\n","eval step: 163\n","eval step: 164\n","eval step: 165\n","eval step: 166\n","eval step: 167\n","eval step: 168\n","eval step: 169\n","eval step: 170\n","eval step: 171\n","eval step: 172\n","eval step: 173\n","eval step: 174\n","eval step: 175\n","eval step: 176\n","eval step: 177\n","eval step: 178\n","eval step: 179\n","eval step: 180\n","eval step: 181\n","eval step: 182\n","eval step: 183\n","eval step: 184\n","eval step: 185\n","eval step: 186\n","eval step: 187\n","eval step: 188\n","eval step: 189\n","eval step: 190\n","eval step: 191\n","eval step: 192\n","eval step: 193\n","eval step: 194\n","eval step: 195\n","eval step: 196\n","eval step: 197\n","eval step: 198\n","eval step: 199\n","eval step: 200\n","eval step: 201\n","eval step: 202\n","eval step: 203\n","eval step: 204\n","eval step: 205\n","eval step: 206\n","eval step: 207\n","eval step: 208\n","eval step: 209\n","eval step: 210\n","eval step: 211\n","eval step: 212\n","eval step: 213\n","eval step: 214\n","eval step: 215\n","eval step: 216\n","eval step: 217\n","eval step: 218\n","eval step: 219\n","eval step: 220\n","eval step: 221\n","eval step: 222\n","eval step: 223\n","eval step: 224\n","eval step: 225\n","eval step: 226\n","eval step: 227\n","eval step: 228\n","eval step: 229\n","eval step: 230\n","eval step: 231\n","eval step: 232\n","eval step: 233\n","eval step: 234\n","eval step: 235\n","eval step: 236\n","eval step: 237\n","eval step: 238\n","eval step: 239\n","eval step: 240\n","eval step: 241\n","eval step: 242\n","eval step: 243\n","eval step: 244\n","eval step: 245\n","eval step: 246\n","eval step: 247\n","eval step: 248\n","eval step: 249\n","eval step: 250\n","eval step: 251\n","eval step: 252\n","eval step: 253\n","eval step: 254\n","eval step: 255\n","eval step: 256\n","eval step: 257\n","eval step: 258\n","eval step: 259\n","eval step: 260\n","eval step: 261\n","eval step: 262\n","eval step: 263\n","eval step: 264\n","eval step: 265\n","eval step: 266\n","eval step: 267\n","eval step: 268\n","Break due to OutOfRangeError exception\n"]}],"source":["# Run one step of inference (on the training set).\n","\n","# Set category names in task for visualization.\n","# The category names for COCO are picked up from the coco annotation files. For\n","# other datasets, they can be added manually in the code. If they are missing,\n","# the visualization will not contain category names for predicted boxes, but\n","# no other things will be impacted.\n","# category_names = [\n","#     'Aeroplane', 'Bicycle', 'Bird', 'Boat', 'Bottle', 'Bus', 'Car', 'Cat',\n","#     'Chair', 'Cow', 'Dining table', 'Dog', 'Horse', 'Motorbike', 'People',\n","#     'Potted plant', 'Sheep', 'Sofa', 'Train', 'TV/monitor']\n","category_names = ['none', 'sunglass', 'hat', 'jacket', 'shirt', 'pants', 'shorts',\n","        'skirt', 'dress', 'bag', 'shoe']\n","task._category_names = {\n","    i + 1 : {'name': name} for i, name in enumerate(category_names)}\n","\n","def single_step(examples):\n","  preprocessed_outputs = task.preprocess_batched(examples, training=False)\n","  infer_outputs = task.infer(trainer.model, preprocessed_outputs)\n","  return task.postprocess_tpu(*infer_outputs)\n","\n","records = []\n","visuals = []\n","eval_steps = 0 #10\n","\n","\n","with strategy.scope():\n","  @tf.function\n","  def run_single_step(iterator):\n","    examples = next(iterator)\n","    outputs = strategy.run(single_step, (examples,))\n","    if outputs is not None:\n","      outputs = [strategy.gather(t, axis=0) for t in outputs]\n","    return outputs\n","\n","\n","  iterator = iter(eval_ds)\n","  cur_step = 0\n","  while True:\n","      print(\"eval step:\", cur_step)\n","      if eval_steps and cur_step >= eval_steps:\n","        break\n","      try:\n","        per_step_outputs = run_single_step(iterator)\n","        vis = task.postprocess_cpu(\n","            per_step_outputs,\n","            train_step=100, # global_step.numpy(),\n","            eval_step=cur_step,\n","            ret_results=True)\n","        \n","        records.append(vis['records'])\n","        visuals.append(vis['pred'])\n","        cur_step += 1\n","      except tf.errors.OutOfRangeError:\n","        print('Break due to OutOfRangeError exception')\n","        break\n","\n","\n","  # per_step_outputs = run_single_step(iterator)\n","  # vis = task.postprocess_cpu(\n","  #     per_step_outputs,\n","  #     train_step=50,\n","  #     eval_step=0,\n","  #     ret_results=True)\n","  # records.append(vis['records'])\n","\n","\n","for record in records:\n","  for key in record.keys():\n","    record[key] = record[key].numpy()\n","  "]},{"cell_type":"code","source":["print(len(records))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJQJb72fSn7C","executionInfo":{"status":"ok","timestamp":1666612792261,"user_tz":-210,"elapsed":24,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"71aa118d-6a4d-4bbb-acde-4b993e3ac396"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["268\n"]}]},{"cell_type":"code","source":["import pickle\n","out_fname = 'resnet50c_eval_10-24-15-07'\n","with open('outputs/'+out_fname+'.pickle', 'wb') as f:\n","    pickle.dump(records, f)\n","  "],"metadata":{"id":"9W7UPnhDVHmg","executionInfo":{"status":"ok","timestamp":1666612792262,"user_tz":-210,"elapsed":13,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import pickle\n","#out_fname = 'resnet50c_eval_10-24-15-07'\n","out_fname = 'resnet50_eval_10-24-12-59'\n","with open('outputs/'+out_fname+'.pickle', 'rb') as f:\n","    loaded_recs = pickle.load(f)\n"],"metadata":{"id":"qmBGYJ2JJ7_D","executionInfo":{"status":"ok","timestamp":1666612973194,"user_tz":-210,"elapsed":1442,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["!pip install mapcalc"],"metadata":{"id":"lWkHzYsSlsOC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mapcalc import calculate_map, calculate_map_range\n","\n","batch_size = config.train.batch_size #8\n","\n","ap = 0\n","ap_50 = 0\n","ap_75 = 0\n","n_samples = 0\n","\n","for rec in loaded_recs:\n","  for i in range(batch_size):\n","    n_boxes = np.count_nonzero(rec['gt_classes'][i])\n","    ground_truth = {\n","      'boxes': rec['gt_bboxes'][i][:n_boxes],\n","      'labels': rec['gt_classes'][i][:n_boxes]}\n","    result_dict = {\n","        'boxes': rec['pred_bboxes'][i][:n_boxes],\n","        'labels': rec['pred_classes'][i][:n_boxes],\n","        'scores': rec['scores'][i][:n_boxes]}\n","    \n","    n_samples += 1\n","    \n","    ap += calculate_map_range(ground_truth, result_dict, 0.5, 0.95, 0.05)\n","    ap_50 += calculate_map(ground_truth, result_dict, 0.5)\n","    ap_75 += calculate_map(ground_truth, result_dict, 0.75)\n","\n","print('Model:', out_fname)\n","print(\"AP: {:.4f}, AP50: {:.4f}, AP75: {:.4f}\".format(ap / n_samples, ap_50 / n_samples, ap_75 / n_samples))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DjujTf6l1Ka","executionInfo":{"status":"ok","timestamp":1666612975104,"user_tz":-210,"elapsed":3,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"41e83e79-5f8d-4255-91a8-4efe74a34a6a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: resnet50_eval_10-24-12-59\n","AP: 0.5616, AP50: 0.7944, AP75: 0.6060\n"]}]},{"cell_type":"code","source":["# summary_writer = tf.summary.create_file_writer('model_dir/')\n","# eval_tag = config.eval.tag\n","\n","# cur_step = global_step.numpy()\n","# result = task.evaluate(summary_writer, cur_step, eval_tag)\n","# result.update({'global_step': cur_step})\n","# print(result)"],"metadata":{"id":"1spUWcjPuZ_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1oIA7LiGLX1EKRqpG9TMcaCz2JBzq-jLR"},"executionInfo":{"elapsed":16625,"status":"ok","timestamp":1666603921003,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"},"user_tz":-210},"id":"QD_rgNA_XQMN","outputId":"bb07bf66-272a-4cfc-de7c-9201b722e27e"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Visualization.\n","vis = visuals[0]\n","im = tf.concat([vis[i] for i in range(config.train.batch_size)], 0)\n","Image.fromarray(np.uint8(im.numpy() * 255))"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Matority/pix2seq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QYP1Fmx357km","executionInfo":{"status":"ok","timestamp":1666593990339,"user_tz":-210,"elapsed":343,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"5c2cbb7d-49f3-490c-a762-5c4dfef5f07b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Matority/pix2seq\n"]}]},{"cell_type":"code","source":["!git checkout master   \n","!git branch main master -f    \n","!git checkout main  \n","!git push origin main -f "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbgQTYdGAYaf","executionInfo":{"status":"ok","timestamp":1666531300443,"user_tz":-210,"elapsed":1548,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"e3b8f36d-743f-43e1-cfdc-e0a6279d14d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["M\tcolabs/Copy of Pix2Seq Finetuning Object Detection.ipynb\n","M\tdata/__pycache__/dataset.cpython-37.pyc\n","M\tmodels/__pycache__/ar_model.cpython-37.pyc\n","M\ttasks/__pycache__/object_detection.cpython-37.pyc\n","Already on 'master'\n","M\tcolabs/Copy of Pix2Seq Finetuning Object Detection.ipynb\n","M\tdata/__pycache__/dataset.cpython-37.pyc\n","M\tmodels/__pycache__/ar_model.cpython-37.pyc\n","M\ttasks/__pycache__/object_detection.cpython-37.pyc\n","Switched to branch 'main'\n","Total 0 (delta 0), reused 0 (delta 0)\n","To https://github.com/mehrdadsaberi/pix2seq-M.git\n"," + 6d45f77...0db54da main -> main (forced update)\n"]}]},{"cell_type":"code","source":["!git init"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJycZ90q2Qxj","executionInfo":{"status":"ok","timestamp":1666529072449,"user_tz":-210,"elapsed":700,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"aa602556-a864-4d6d-9a83-4b3a2854d462"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized empty Git repository in /content/drive/MyDrive/Matority/pix2seq/.git/\n"]}]},{"cell_type":"code","source":["!git remote set-url origin https://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@github.com/mehrdadsaberi/pix2seq-M.git"],"metadata":{"id":"BsR44OkY1i9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"outputs fixed\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aia8lxHz6CoI","executionInfo":{"status":"ok","timestamp":1666593985230,"user_tz":-210,"elapsed":2979,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"5d40851b-1494-48a8-ad4e-a5b84861f2e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: Unable to create '/content/drive/MyDrive/Matority/pix2seq/.git/index.lock': File exists.\n","\n","Another git process seems to be running in this repository, e.g.\n","an editor opened by 'git commit'. Please make sure all processes\n","are terminated then try again. If it still fails, a git process\n","may have crashed in this repository earlier:\n","remove the file manually to continue.\n"]}]},{"cell_type":"code","source":["!git push origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9AsMOjzP6YAg","executionInfo":{"status":"ok","timestamp":1666531357806,"user_tz":-210,"elapsed":1100,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"22f9f04e-bd74-4524-bcf7-3d58f814dc5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Counting objects: 13, done.\n","Delta compression using up to 2 threads.\n","Compressing objects:   7% (1/13)   \rCompressing objects:  15% (2/13)   \rCompressing objects:  23% (3/13)   \rCompressing objects:  30% (4/13)   \rCompressing objects:  38% (5/13)   \rCompressing objects:  46% (6/13)   \rCompressing objects:  53% (7/13)   \rCompressing objects:  61% (8/13)   \rCompressing objects:  69% (9/13)   \rCompressing objects:  76% (10/13)   \rCompressing objects:  84% (11/13)   \rCompressing objects:  92% (12/13)   \rCompressing objects: 100% (13/13)   \rCompressing objects: 100% (13/13), done.\n","Writing objects:   7% (1/13)   \rWriting objects:  15% (2/13)   \rWriting objects:  23% (3/13)   \rWriting objects:  30% (4/13)   \rWriting objects:  38% (5/13)   \rWriting objects:  46% (6/13)   \rWriting objects:  53% (7/13)   \rWriting objects:  61% (8/13)   \rWriting objects:  69% (9/13)   \rWriting objects:  76% (10/13)   \rWriting objects:  84% (11/13)   \rWriting objects:  92% (12/13)   \rWriting objects: 100% (13/13)   \rWriting objects: 100% (13/13), 13.25 KiB | 1.66 MiB/s, done.\n","Total 13 (delta 7), reused 0 (delta 0)\n","remote: Resolving deltas:   0% (0/7)\u001b[K\rremote: Resolving deltas:  14% (1/7)\u001b[K\rremote: Resolving deltas:  28% (2/7)\u001b[K\rremote: Resolving deltas:  42% (3/7)\u001b[K\rremote: Resolving deltas:  57% (4/7)\u001b[K\rremote: Resolving deltas:  71% (5/7)\u001b[K\rremote: Resolving deltas:  85% (6/7)\u001b[K\rremote: Resolving deltas: 100% (7/7)\u001b[K\rremote: Resolving deltas: 100% (7/7), completed with 7 local objects.\u001b[K\n","To https://github.com/mehrdadsaberi/pix2seq-M.git\n","   0db54da..b3b0e9c  main -> main\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"merhdads@gmail.com\"\n","!git config --global user.name \"mehrdadsaberi\""],"metadata":{"id":"A9jJpDRE6N_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -rf ~"],"metadata":{"id":"ytC94MHDvxXf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add . -v"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kvOXx4P_4nWS","executionInfo":{"status":"ok","timestamp":1666594137892,"user_tz":-210,"elapsed":344,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"a6872d97-0294-499c-b37b-4c71122c9fad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: not a git repository (or any parent up to mount point /content)\n","Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rb5crypyKn3v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666529641783,"user_tz":-210,"elapsed":348,"user":{"displayName":"Mehrdad Saberi","userId":"11186075728833307216"}},"outputId":"f502fea7-3890-4ede-dce1-77af9e544dd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["105K\tarchitectures\n","3.3M\tcolabs\n","22K\tconfigs\n","1.5K\tCONTRIBUTING.md\n","81K\tdata\n","12K\tLICENSE\n","22K\tmetrics\n","55K\tmodels\n","12M\tpix2seq.gif\n","237K\tpix2seq.png\n","16K\t__pycache__\n","8.0K\tREADME.md\n","1.5K\tregistry.py\n","512\trequirements.txt\n","9.5K\trun.py\n","7.0K\tsample_run.py\n","215K\ttasks\n","12K\tutils.py\n","2.0K\tvocab.py\n","16M\ttotal\n"]}],"source":["!du -shc *"]}],"metadata":{"colab":{"collapsed_sections":[],"last_runtime":{},"provenance":[{"file_id":"https://github.com/google-research/pix2seq/blob/master/colabs/pix2seq_finetuning_object_detection.ipynb","timestamp":1666522575061}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}